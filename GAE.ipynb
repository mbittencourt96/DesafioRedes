{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando os grafos e csv necessários e bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import random\n",
    "from torch_geometric.nn import GAE, GCNConv\n",
    "import torch.nn as nn\n",
    "from torch import nn\n",
    "import torch_geometric.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch \n",
    "from torch_geometric import seed_everything\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "G = nx.read_gml('GraphMissingEdges.gml')\n",
    "\n",
    "edges_to_evaluate = pd.read_csv('edgesToEvaluate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(69)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "categories = pd.read_csv('categories.csv', index_col='CategoryId')\n",
    "num_categorias = len(categories)\n",
    "\n",
    "def bag_of_words(categories_list):\n",
    "    # print(categories_list)\n",
    "    categorias = categories_list.split(',')\n",
    "    bag = np.zeros(num_categorias, dtype=np.float32)\n",
    "\n",
    "    for i, categoria in enumerate(categorias):\n",
    "        try:\n",
    "            bag[i] = 1\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return bag\n",
    "\n",
    "G_pyg = G.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o dataset a partir do grafo do desafio\n",
    "# Aqui definimos os parametros que serão relevantes (rating, reviewCount e categories)\n",
    "\n",
    "key_to_index = {}\n",
    "for index, (n, data) in enumerate(G_pyg.nodes.data()):\n",
    "    key_to_index[n] = index\n",
    "    G_pyg.nodes[n]['stars'] = np.array([float(data['stars'])])\n",
    "    G_pyg.nodes[n]['reviewCount'] = np.array([float(data['reviewCount'])])\n",
    "    G_pyg.nodes[n]['categories'] = bag_of_words(data['categories'])\n",
    "\n",
    "dataset = from_networkx(G_pyg, group_edge_attrs=all,group_node_attrs=['categories','reviewCount','stars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição da GAE de duas camadas, ativação com Relu na GCN, otimizador Adam, learning rate = 0.001 e 100 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parâmetros\n",
    "out_channels = 128\n",
    "num_features = dataset.num_features\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instancia o modelo - Graph Auto-Encoder (GAE)\n",
    "model = GAE(GCNEncoder(num_features, out_channels))\n",
    "model = model.to(device)\n",
    "\n",
    "# otimizador\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 30386], longitude=[4575], latitude=[4575], name=[4575], x=[4575, 895], edge_attr=[30386, 1], pos_edge_label=[15193], pos_edge_label_index=[2, 15193], neg_edge_label=[15193], neg_edge_label_index=[2, 15193])\n",
      "-----\n",
      "Data(edge_index=[2, 34184], longitude=[4575], latitude=[4575], name=[4575], x=[4575, 895], edge_attr=[34184, 1], pos_edge_label=[1899], pos_edge_label_index=[2, 1899], neg_edge_label=[1899], neg_edge_label_index=[2, 1899])\n",
      "\n",
      "Data(edge_index=[2, 30386], longitude=[4575], latitude=[4575], name=[4575], x=[4575, 895], edge_attr=[30386, 1], pos_edge_label=[15193], pos_edge_label_index=[2, 15193], neg_edge_label=[15193], neg_edge_label_index=[2, 15193])\n",
      "Data(edge_index=[2, 30386], longitude=[4575], latitude=[4575], name=[4575], x=[4575, 895], edge_attr=[30386, 1], pos_edge_label=[1899], pos_edge_label_index=[2, 1899], neg_edge_label=[1899], neg_edge_label_index=[2, 1899])\n"
     ]
    }
   ],
   "source": [
    "dataset.train_mask = None\n",
    "dataset.val_mask = None\n",
    "dataset.test_mask = None\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.RandomLinkSplit(num_val=0.10, num_test=0.10, neg_sampling_ratio = 1.0,\n",
    "                  is_undirected=True, add_negative_train_samples=True, split_labels=True),\n",
    "])\n",
    "\n",
    "train_data, val_data, test_data = transform(dataset)\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "print(train_data)\n",
    "print('-----')\n",
    "print(test_data)\n",
    "print()\n",
    "print(train_data)\n",
    "print(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under the Curve: 0.8288902920275381, AP: 0.8627452026622962\n"
     ]
    }
   ],
   "source": [
    "x = train_data.x.to(device).to(torch.float)\n",
    "train_pos_edge_index = train_data.pos_edge_label_index.to(device)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return float(loss)\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "        out = model.decode(z, train_pos_edge_index)\n",
    "        return model.test(z, pos_edge_index, neg_edge_index)\n",
    "\n",
    "# Treino\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train()\n",
    "    auc, ap = test(val_data.pos_edge_label_index, val_data.neg_edge_label_index)\n",
    "    # print('Época: {:}, Perda: {:}, AUC: {:}, AP: {:}'.format(epoch, loss, auc, ap))   \n",
    "\n",
    "# Teste\n",
    "auc, ap = test(test_data.pos_edge_label_index, test_data.neg_edge_label_index)\n",
    "print('Area Under the Curve: {:}, AP: {:}'.format( auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodificação do teste expandida\n",
    "temp_positivos = model.decode(test_data.x, test_data.pos_edge_label_index)\n",
    "temp_negativos = model.decode(test_data.x, test_data.neg_edge_label_index)\n",
    "\n",
    "# resultados\n",
    "results = torch.cat([temp_positivos, temp_negativos]).cpu()\n",
    "labels = torch.cat([test_data.pos_edge_label, test_data.neg_edge_label]).cpu()\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(labels, results)\n",
    "f1_scores = 2*recall*precision/(recall+precision)\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "threshold = best_threshold\n",
    "\n",
    "evaluation_edges_u = []\n",
    "evaluation_edges_v = []\n",
    "for _, linkID, u, v in edges_to_evaluate.to_records():\n",
    "    u_idx = key_to_index[u]\n",
    "    v_idx = key_to_index[v]\n",
    "    evaluation_edges_u.append(key_to_index[u])\n",
    "    evaluation_edges_v.append(key_to_index[v])\n",
    "\n",
    "edges_to_evaluate_tensor = torch.tensor([evaluation_edges_u, evaluation_edges_v])\n",
    "\n",
    "# decoder\n",
    "temp = model.decode(test_data.x, edges_to_evaluate_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribui 0 ou 1 de acordo com o threshold\n",
    "resultados = (temp > threshold).int()\n",
    "# detach pra cpu pra poder usar o pandas\n",
    "resultados = resultados.cpu().detach()\n",
    "\n",
    "concatenados = pd.concat([edges_to_evaluate, pd.Series(resultados, name='link')], axis=1)\n",
    "links = concatenados[['linkID', 'link']]\n",
    "links.to_csv('results_gae_teste4.csv', columns=['linkID', 'link'],index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}